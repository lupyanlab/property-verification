---
title: "Replication of `question_first` experiment"
author: "Pierce Edmiston"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    toc: true
---

```{r, config, echo = FALSE}
library(knitr)
opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.path = "replication-figs/"
)

read_chunk("replication.R")
```

```{r, setup}
```

# Conclusions

- We didn't replicate the effect. The most likely conclusion is that we
  got lucky the first time, or at the very least the effect is more
  fragile than our theory predicts.
- However, the effects are significantly different between the two runs
  (a three-way interaction between feature x mask x run), which at
  least suggests it would be worth ruling out that there was a difference
  in the methods or the participants.
- For instance, it seems that the trial generation file behaved differently
  at the time of the first run compared to the second run, even though there was
  no difference line-by-line between the trial generation files (see below).
  My best guess is that the first run was done using an old version of pandas 
  that got updated by the time the second run was conducted.
- One consequence of the different trials files, other than a different number
  of total trials per subject, was that some questions were unintentionally seen
  more than once in the second run. Removing the duplicated questions did not
  revive the effect.
- Participants in the second run were not any slower overall than participants
  in the first run, and if anything they were more accurate, so we don't have
  any evidence to support the claim that the end of the semester subjects
  were sloppier than the first run subjects.

# First run

Here is the effect we were trying to replicate.

```{r, run1-mod, echo = 1}
```

```{r, run1-plot}
```

# Second run

In the second run of the experiment, we didn't find the same effect of
the mask. Instead we found an overall effect of question type such
that visual questions were more difficult than nonvisual questions.

```{r, run2-mod, echo = 1}
```

```{r, run2-plot}
```

# Overall

Across both runs the detrimental effect of the mask was not greater
for the visual questions than it was for the nonvisual questions. Instead,
the overall error rate for visual questions increased so that there
is an overall difference between visual and nonvisual questions.

```{r, mod, echo = 1}
```

```{r, plot}
```

# Differences between runs

What was different about the two runs of the experiment? First,
we should test that the effect is indeed different between
the two runs.

It should be noted that this analysis is post-hoc and exploratory,
and there were no **intended** differences between the first and
second run of the experiment, so the most likely source of the
failure-to-replicate is that the preliminary finding was type-2 error.

## Was the effect stronger in the first run?

The effect was indeed stronger in the first run than it was in the second run.

```{r, run-diff-mod, echo = 1}
```

In this plot, each point is a subject, and plotted along the y-axis is the
interaction term for that subject: the extent to which they show the expected
`feature x mask` interaction.

```{r, run-diff-plot}
```

## Were there differences in overall error rates or RTs?

There is a marginal difference in overall error rates such that
errors were less likely in the second run of the experiment.

```{r, overall-error}
```

There were no differences in overall RTs across the experiment runs.

```{r, overall-rt}
```

### By-subject averages

There don't appear to be any outlier subjects in the second run.

```{r, subjs}
```

## Did the same questions get used in both runs?

Although there were equal numbers of true and false propositions in the 
stimuli, each participant only saw a subset of those propositions, and 
not in equal proportion. Participants saw 75% true propositions and 25%
false ones.

```{r, question-freq}
```

Of the 500+ questions that could have been sampled for each subject,
only these 10 appeared in one run but not the other, so it seems
that for the most part the same questions were used in the two runs,
and these 10 appearing in only one version by chance.

```{r, same-propositions}
```

## Did any subjects see the same question multiple times?

It seems that for some reason the trials for the second run of the experiment
were different and some questions appeared more than one time. This was true
for all subjects in the second run.

```{r, unique-propositions}
```

### Effect of mask on error rate (duplicates dropped)

Redoing the main analyses without the duplicated trials, however, does not
change the original conclusions.

```{r, drop-duplicates}
```

### Effect of duplicated questions on error rate

Error rate on the duplicated questions was more than double what it was
when the question was seen the first time, indicating that seeing a question
multiple times encouraged different response patterns than when it was
seen the first time.

```{r, error-rate-on-duplicated-questions, echo = TRUE}
```

## How did the python files differ?

### Experiment files

`diff -b diff/first_run/MWPF2.py diff/second_run/question_first.py`

```{r, eval = FALSE, echo = TRUE}
# the first run had a comment at the top of the experiment script
2,4d1
< """
< MWP-F/run.py
< """

# the second run set the psychopy audioLib manually
9a7,12
> from psychopy import prefs
> try:
>     import pyo
> except ImportError:
>     print 'pyo not found!'
> prefs.general['audioLib'] = ['pyo', ]
10a14
> sound.init(48000, buffer=128)

# the trials files were named differently, so they were imported differently
12d15
< from MWPF1_trials import write_trials
15a19,20
> from question_first_trials import write_trials
> 
  
# self.trials_fields is a tuple so it needed to be converted to
# a list before it could be used in the concatenation of the header
# file. The header file was only created once, so the bug didn't
# cause a problem in the first run (though it could have). This
# bug was fixed in the second run.
206c211
<             header = self.subj_vars_fields+self.trials_fields+self.resp_fields
---
>             header = self.subj_vars_fields+list(self.trials_fields)+self.resp_fields

# different config files were loaded for the first and second run experiments
335c340
<     exp = Experiment(exp_dir=root_dir, version_file='MWPF2.yaml')
---
>     exp = Experiment(exp_dir=root_dir, version_file='question_first.yaml')
```

### Trials files

The only difference between the trials files was that the two runs took
in a differently named config files.

`diff -b diff/first_run/MWPF2_trials.py diff/second_run/question_first_trials.py`

```{r, eval = FALSE, echo = TRUE}
138c138
<     version_file = os.path.join(exp_dir, 'MWPF2.yaml')
---
>     version_file = os.path.join(exp_dir, 'question_first.yaml')
```

### Config files

As can be seen in the config file below, more categories were used
in the second run of the experiment than were used in the first run.

`diff diff/first_run/MWPF2.yaml diff/second_run/question_first.yaml`

```{r, eval = FALSE, echo = TRUE}
# the different runs used different names
1,2c1,2
< task_name: "Masked Word Priming: Features 2"
< version_name: "MWPF2"
---
> task_name: "Property Verification: Question First"
> version_name: "question_first"

# each participant was assigned questions from 18 categories
# in the first run of the experiment, and 33 categories in
# the second run.
23c23
<     num_categories: 18
---
>     num_categories: 33
```

This is an interesting clue because I think it reflects that there
was a difference between the two experiments, likely in terms of the
version of pandas installed on the testing computers.

Here's why: when we started the second run of participants, the
number of categories number had not changed from the first run,
i.e., it was still 18. However, after the first few subjects,
we noticed that the subjects weren't completing as many trials
as they were in the first run. This was weird because the trials
file was exactly the same (as seen above).

First I confirmed that the number of trials was indeed different,
but then I tried to create a new trial list using the old version
of the trials file and came up with a shorter trials file than
was already recorded in the first run. This really could only
be explained by a difference between pandas functionality.

To compensate because we were short on time I simply increased the number
of categories in the second run of the experiment in order to give
participants more trials.