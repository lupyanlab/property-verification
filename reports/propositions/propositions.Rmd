---
title: "Differences among propositions"
author: "Pierce Edmiston"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    toc: true
---

```{r, config, echo = FALSE}
library(knitr)
opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.path = "propositions-figs/"
)
read_chunk("setup.R")
read_chunk("ambiguous.R")
read_chunk("incorrectly-coded.R")
read_chunk("baseline-difficulty.R")
read_chunk("subjective-objective-difficulty.R")
read_chunk("subset.R")
```

```{r, setup, echo = 1:5}
```

# Ambiguous propositions

Ideally, people would agree as to the truth or falseness of each of
the 536 propositions we created for this experiment, yet against our best 
intentions, there were some propositions for which people did not agree
on an answer.

To determine which propositions were ambiguous, I fit a linear model to the
responses for each proposition given in the norming study. In the norming
study, participants were asked to respond to the yes or no question on a
5-point scale from _Definitely no_ to _Definitely yes_. The scale was centered
on _Maybe_, corresponding to a truth value of 0. The linear models predicted
mean truth value for each proposition, and the statistical test of interest
was whether this value differently reliably from 0 (the intercept term),
which would indicate that people tended to agree on the truth or falseness
of each of the propositions.

Any proposition for which mean truth value did not significantly differ from 0
(_p_ < 0.05) was deemed "ambiguous". If the truth value of the proposition is
ambiguous as determined by the norming participants, then supposedly incorrect
responses to these propositions in the main experiment cannot be interpreted as
errors in the actual experiment.

```{r, ambiguous}
```

```{r, export-ambiguous}
```

# Incorrectly-coded propositions

With the agreement scores above, we can also check whether the correct response
as coded in the experiment matches what the norming participants thought was
the correct answer. Basically, if there are any cases that the correct response
was not the same as the norm truth value, we would conclude that these 
propositions were miscoded.

Fortunately of the questions that were not ambiguous, all of the questions
were correctly coded.

```{r, incorrectly-coded}
```

# Proposition difficulty

To determine which propositions were too difficult, I fit a model predicting
error rate on the nomask trials for each proposition. Presumably, college
undergraduates should be able to to answer these questions correctly without
any distractions, and therefore we would expect most of the propositions to
have a reliably negative intercept term. A negative log-odds value when error
rate is being predicted is interpreted as the question being easy to answer:
errors were rare.

A positive log-odds value for the estimate indicates that errors were more
likely than would be expected. More conservatively, a log-odds value that was
*not* significantly negative is used as an indicator that the proposition
was simply too difficult because too many people were getting it wrong.

The plot below is of the significance value of the intercept terms for each
of these models. Propositions with positive estimates, or estimates that were
not significantly different from chance, were labeled as being too hard.

```{r, baseline-difficulty}
```

```{r, export-difficult}
```

# Subjective and objective measures of proposition difficulty

Subjective proposition difficulty is measured as the mean difficulty rating
given to the proposition by participants in the norming study.

Objective proposition difficulty is measured as mean performance on the
proposition (error rate and reaction time) in the experiment **on the
no mask trials only**.

```{r, subj-obj-diff}
```

```{r, subj-obj-diff-by-correctness}
```

# Subsetting balanced propositions

```{r, subset}
```

```{r, export-subset}
```